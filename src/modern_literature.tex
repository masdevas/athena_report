\subsection{Обзор современной литературы}
	Рост популярности искусственного интеллекта привел к появлению большого количества инструментов для исследователей в данной области. В частности, фреймворков. Одним из первых таких фреймворков стал Theano, описанная в статье Rami Al-Rfou et al “Theano: A Python framework for fast computation of mathematical expressions”\cite{theano} . Библиотека позволяет определять символьные математические выражения и компилировать их в высоко оптимизированный код для выполнения на CPU и GPU. Выражения представляются в виде графа вычислений - направленного ациклического графа, в котором есть два типа вершин: переменные (variables), содержащие данные, и применяемые (apply nodes), содержащие математические операции. Для дифференцирования графа применяется цепное правило. Также существуют специальные операции, позволяющие управлять потоком выполнения (циклы). Для их дифференцирования применяется техника обратного распространения ошибки во времени, то есть в виде обратного цикла. На этапе обхода графа компилятор может применять к выражению различные оптимизации, например, упрощая выражение, освобождая более не используемые участки памяти.
\par
К сожалению библиотека Theano не обладает возможностями распараллеливания вычислений на кластерах. Решить эту проблему попытались исследователи He Ma, Fei Mao и Graham W. Taylor. В своей статье “Theano-MPI: a Theano-based Distributed Training Framework”\cite{theanompi} они рассмотрели проблемы распараллеливания процесса обучения на нескольких GPU. Графические процессоры могут быть установлены как на локальной машине, так и на удаленной. Чтобы повысить производительность в первом случае, авторы разработали GPU-aware MPI. Данные передаются напрямую от одной видеокарты другой, минуя оперативную память.
\par
Свой фреймворк описали исследователи из Калифорнийского университета в Беркли в статье Yangqing Jia et al “Caffe: Convolutional Architecture for Fast Feature Embedding”\cite{caffe}. Данные в библиотеке представляются в виде капель (blobs) - четырёхмерных массивов. Данные передаются между слоями, которые принимают одну или несколько капель и возвращают одну или несколько. Выполняется два прохода: прямой, во время которого вычисляется значение функции, и обратный, во время которого вычисляется градиент и корректируются веса. Фреймворк разработан таким образом, чтобы быть модульным и переносимым.
\par
Одним из главных игроков на рынке искусственного интеллекта является Google. В статье Jeffrey Dean et al “Large Scale Distributed Deep Networks”\cite{lsd} рассматривают некоторые аспекты работы фреймврока DistBelief, предшественника Tensorflow. Исследователи описывают параллелизм двух видов: на уровне данных и на уровне модели. Для тренировки особо крупных моделей фреймворк может разделить граф вычислений на части и выполнять его параллельно на кластере. При распараллеливании данных главной проблемой является оптимизация функции. Классический алгоритм SGD требует вычисления значений функции для всего массива данных. Авторы предлагают модификацию алгоритма под названием Downpour SGD. Базовый принцип его работы таков: данные делятся на части, каждая машина обрабатывает свою порцию независимо, после чего все данные отправляются на сервер параметров, где происходит их слияние. Перед началом обработки каждой новой порции машина загружает с сервера актуальную версию параметров модели.
\par
DistBelief был внутренней разработкой компании. Но в 2015 году Google представила открытую библиотеку Tensorflow. Её устройство описано в статье Martín Abati et al “TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems”\cite{tf}. Она многое унаследовала от Theano. Вычисления в фреймворке описываются с помощью графа вычислений, который состоит из набора вершин. Для ветвлений и циклов используются структуры внутри графа. У каждой вершины ноль или более входов и ноль или более выходов. Данные передаются по ребрам графа в виде тензоров - строго типизированных массивов произвольной размерности. Также граф может содержать специальные ребра, управляющие зависимостями. По ним не передаются данные, но они показывают, что вычисление одной вершины должно начаться после завершения вычисления другой. В вершинах графа хранятся операции, у которых есть имя и абстрактное представление вычислений (например, сложение, умножение). У них могут быть атрибуты, которые должны быть предоставлены во время построения графа. С каждой операцией связано ядро - конкретная реализация операции. Клиентские приложения могут взаимодействовать с библиотекой путем создания сессии, которая инкапсулирует в себе граф и методы выполнения вычислений на нем. Она же отвечает за управление многопоточностью.
\par
Совершенно иной подход предлагают авторы библиотеки Chainer. Она описана в статье Seiya Tokui et al “Chainer: a Next-Generation Open Source Framework for Deep Learning”\cite{chainer}. Современные фреймворки для глубокого обучения работают по модели Define-and-Run, когда сперва задается граф вычислений, который затем и будет исполняться. Авторы отмечают три проблемы у этого подхода:
\begin{enumerate}
	\item Неэффективное использование памяти. Граф должен оставаться в памяти на протяжении всего процесса обучения.
	\item Ограниченная расширяемость. Для обеспечения обратной совместимости разработчики не могут расширять модель Define-and-Run, чтобы проектировать более сложные нейронные сети.
	\item Внутреннее устройство модели недоступно пользователю. Усложняется отладка нейронной сети.
\end{enumerate}
Чтобы преодолеть эти трудности, авторы представили модель Define-by-Run, когда граф определяется непосредственно в момент его выполнения.
\par
На сегодняшний день большое практическое значение имеет возможность запуска фреймворка на многопоточных и многопроцессорных системах. Этот аспект затрагивают Linpeng Tang, Yida Wang, Theodore L. Willke и Kai Li в статье “Scheduling Computation Graphs of Deep Learning Models on Manycore CPUs”\cite{sched1}. В статье рассматривается влияние процесса планирования потоков на скорость выполнения графа вычислений в нейронных сетях. Авторы отмечают, что большинство известных фреймворков для машинного обучения используют OpenMP или стандартные средства ОС для управления потоками. Из-за этого возникает ситуация, когда виртуальных потоков запущено больше, чем доступно физических. Частые переключения контекста снижают производительность. Подход авторов заключается в создании некой центральной очереди, куда попадают операции, уже готовые для выполнения. Библиотека сама распределяет элементы очереди по потокам. Во время нескольких первых итераций специальный профилировщик собирает информацию, которая в дальнейшем используется при принятии решений о переключении контекста. В результате авторам удалось получить ускорение от 2,1 до 9,5 раз на 68-ядерном процессоре Intel Xeon Phi при обучении 4 популярных архитектур нейронных сетей.
\par
Вопросами производительности озадачены и разработчики компании Facebook. В статье Nadav Rotem et al “Glow: Graph Lowering Compiler Techniques for Neural Networks”\cite{glow} описывается фреймворк Glow. Glow не является самостоятельной библиотекой для искусственного интеллекта. Она предоставляет возможность компиляции графа вычислений в исполняемый код. Сперва она преобразует граф вычислений в высокоуровневый байткод. Операции разбиваются на более мелкие операции линейной алгебры. Над байткодом производится ряд оптимизаций. Затем генерируется биткод LLVM. Для увеличения переносимости вместе с Glow поставляется стандартная библиотека. Она также преобразуется в биткод LLVM и линкуется с графом. Полученный биткод может быть скомпилирован в машинный код для любой платформы, которые поддерживает LLVM.
